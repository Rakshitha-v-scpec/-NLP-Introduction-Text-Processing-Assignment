{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions**: Carefully read each question. Use Google Docs, Microsoft Word, or a similar tool\n",
        "to create a document where you type out each question along with its answer. Save the\n",
        "document as a PDF, and then upload it to the LMS. Please do not zip or archive the files before\n",
        "uploading them. Each question carries 20 marks."
      ],
      "metadata": {
        "id": "JiTwY0j8CphE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "\n",
        "Computational Linguistics (CL) is the scientific study of language using computational methods. It focuses on modeling natural language mathematically and algorithmically so that computers can analyze, understand, and generate human language. CL sits at the intersection of linguistics, computer science, cognitive science, and AI.\n",
        "\n",
        "What Computational Linguistics Does\n",
        "\n",
        "\n",
        "\n",
        "- Builds formal models of syntax, semantics, phonology, morphology, and discourse.\n",
        "\n",
        "- Studies how humans process language and how to replicate or simulate that in computers.\n",
        "\n",
        "- Develops algorithms for parsing, grammar checking, speech recognition, translation, etc.\n",
        "\n",
        "\n",
        "Relationship to NLP\n",
        "\n",
        "Natural Language Processing (NLP) is the engineering and application-oriented side of language technologies. It uses insights from computational linguistics to build practical systems.\n",
        "\n",
        "In short:\n",
        "\n",
        "- Computational Linguistics = the science behind language computation.\n",
        "\n",
        "- NLP = the technology built from that science."
      ],
      "metadata": {
        "id": "X5foJQ3bCpkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "History:\n",
        "\n",
        "   1. 1950s–1960s: Early Rule-Based Systems\n",
        "\n",
        "    Inspired by Alan Turing’s work and early AI research.\n",
        "\n",
        "    Focus on machine translation (e.g., Georgetown-IBM experiment, 1954).\n",
        "\n",
        "    Systems relied on hand-crafted grammar rules and dictionaries.\n",
        "\n",
        "    Progress slowed after the 1966 ALPAC report, which criticized MT results.\n",
        "\n",
        "  2. 1970s–1980s: Linguistic and Knowledge-Based Approaches\n",
        "\n",
        "    Development of formal grammars, parsers, and semantic networks.\n",
        "\n",
        "    Systems like SHRDLU demonstrated limited but deep understanding in small domains.\n",
        "\n",
        "    Emphasis on encoding linguistic and world knowledge explicitly.\n",
        "\n",
        "  3. 1990s: Statistical Revolution\n",
        "\n",
        "    Shift from rules to data-driven methods due to larger corpora and faster computers.\n",
        "\n",
        "    Introduction of probabilistic models: Hidden Markov Models, n-grams, decision trees.\n",
        "\n",
        "    Major applications: speech recognition, part-of-speech tagging, early MT.\n",
        "\n",
        "  4. 2000s: Machine Learning Expansion\n",
        "\n",
        "    Use of supervised learning, SVMs, CRFs.\n",
        "\n",
        "    NLP tasks standardized with large datasets (e.g., Penn Treebank).\n",
        "\n",
        "    Improvements in information extraction, parsing, sentiment analysis.\n",
        "\n",
        "  5. 2010s: Deep Learning Era\n",
        "\n",
        "    Neural networks transform NLP—especially with word embeddings (Word2Vec, GloVe).\n",
        "\n",
        "    RNNs, LSTMs, CNNs dominate tasks like speech recognition and MT.\n",
        "\n",
        "    2017: Transformers introduced (Vaswani et al.), enabling large-scale pretraining.\n",
        "\n",
        "  6. 2020s–Present: Large Language Models (LLMs)\n",
        "\n",
        "    Massive pretrained models (GPT, BERT, T5, LLaMA, etc.) trained on broad corpora.\n",
        "\n",
        "    Achieve near-human performance in many tasks: reasoning, generation, translation.\n",
        "\n",
        "    Increasing focus on alignment, safety, multimodality, and efficiency."
      ],
      "metadata": {
        "id": "IqYAJZjdDRKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "    1. Chatbots and Virtual Assistants\n",
        "\n",
        "        NLP powers systems like customer-service chatbots, Siri, Alexa, and Google Assistant.\n",
        "        It enables:\n",
        "\n",
        "        Understanding user queries (intent detection)\n",
        "\n",
        "        Extracting key information (entities like names, dates)\n",
        "\n",
        "        Generating natural, human-like responses\n",
        "        This helps companies automate support, reduce costs, and provide 24/7 service.\n",
        "\n",
        "\n",
        "\n",
        "    2. Machine Translation\n",
        "\n",
        "        Tools like Google Translate and DeepL rely on NLP to convert text or speech from one language to another.\n",
        "        Modern translation uses deep learning and large language models to:\n",
        "\n",
        "        Understand context\n",
        "\n",
        "        Retain meaning across languages\n",
        "\n",
        "        Produce fluent, natural translations\n",
        "        This is crucial for global communication, international business, and multilingual content.\n",
        "\n",
        "    \n",
        "    3. Sentiment Analysis\n",
        "\n",
        "        Companies use NLP to automatically detect opinions and emotions in text from:\n",
        "\n",
        "        Social media posts\n",
        "\n",
        "        Product reviews\n",
        "\n",
        "        Customer feedback\n",
        "        Sentiment analysis helps businesses understand public perception, track brand reputation, and make data-driven decisions."
      ],
      "metadata": {
        "id": "0taPO8gfD4vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "    Text normalization is the process of converting text into a consistent, standard, and machine-readable form. It prepares raw, messy text so that NLP models can process it accurately.\n",
        "\n",
        "    Common text normalization steps\n",
        "\n",
        "    Lowercasing (e.g., “Apple” → “apple”)\n",
        "\n",
        "    Removing punctuation or special characters\n",
        "\n",
        "    Expanding contractions (“don’t” → “do not”)\n",
        "\n",
        "    Lemmatization or stemming (“running” → “run”)\n",
        "\n",
        "    Standardizing spelling or formats (e.g., dates, numbers)\n",
        "\n",
        "    Handling slang or informal text (“u” → “you”)\n",
        "\n",
        "\n",
        "  - Why is text normalization essential?\n",
        "\n",
        "    Reduces variation in text\n",
        "    Similar words are converted to a common form, reducing noise and improving model performance.\n",
        "\n",
        "    Improves consistency across data\n",
        "    Models treat equivalent words or structures the same way, which boosts accuracy in tasks like classification or clustering.\n",
        "\n",
        "    Enhances downstream NLP tasks\n",
        "    Tasks such as sentiment analysis, machine translation, and search retrieval become more reliable because the input is cleaner and more uniform.\n",
        "\n",
        "\n",
        "Text normalization ensures that raw textual data is uniform, structured, and easier for NLP systems to understand and process effectively."
      ],
      "metadata": {
        "id": "XfNdeFAkEYqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare and contrast stemming and lemmatization with suitable\n",
        "examples.\n",
        "\n",
        "    1. Stemming\n",
        "\n",
        "        Definition:\n",
        "        A rule-based process that chops off word endings to reduce a word to its stem, which may not be a valid dictionary word.\n",
        "\n",
        "        Characteristics:\n",
        "\n",
        "        Fast and simple\n",
        "\n",
        "        Often crude; may produce non-words\n",
        "\n",
        "        Ignores context and part-of-speech\n",
        "\n",
        "\n",
        "       -  Original word -----\tStemmed form\n",
        "        - running -----------\trun / runn (depending on stemmer)\n",
        "        - studies -----------\tstudi\n",
        "        - better ------------\tbetter (unchanged—no rule for this)\n",
        "\n",
        "        Use case: When speed matters and perfect accuracy isn’t required (e.g., search engines).\n",
        "\n",
        "\n",
        "    2. Lemmatization\n",
        "\n",
        "        Definition:\n",
        "        A linguistically informed process that reduces a word to its lemma—its dictionary form—using vocabulary and morphological analysis.\n",
        "\n",
        "        Characteristics:\n",
        "\n",
        "        More accurate\n",
        "\n",
        "        Produces valid words\n",
        "\n",
        "        Considers part-of-speech and context\n",
        "\n",
        "       -  Original word  -\tLemma (with POS)\n",
        "        - running (verb) -\trun\n",
        "        - studies (noun) -\tstudy\n",
        "        - better (adj.)\t - good\n",
        "\n",
        "\n",
        "    Stemming = quick, mechanical cutting of word endings\n",
        "\n",
        "    Lemmatization = precise, linguistically meaningful reduction of a word to its base form"
      ],
      "metadata": {
        "id": "tzeCFeHEE0Cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”"
      ],
      "metadata": {
        "id": "YYQ23QlqF-fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "# Regex pattern for extracting email addresses\n",
        "pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}'\n",
        "\n",
        "# Extract all matches\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "# Print results\n",
        "print(\"Extracted email addresses:\")\n",
        "for email in emails:\n",
        "    print(email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfdqi72UGJUy",
        "outputId": "e28fba20-05db-49f3-86b6-e8e393c1f2a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted email addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical."
      ],
      "metadata": {
        "id": "ELVDbeXpGX9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Sample paragraph\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Download tokenizer resources (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, count in freq_dist.items():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "id": "PMQyKIMqGomI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLP: 3\n",
        "language: 1\n",
        "is: 1\n",
        "and: 2\n",
        "...\n"
      ],
      "metadata": {
        "id": "JZpidWhvG47m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given text.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "W5e2dJDfG6IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- Using spaCy to Identify and Label Proper Nouns -----------\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Barack Obama met Elon Musk in California to discuss SpaceX projects.\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Custom annotator: extract and label proper nouns (PROPN)\n",
        "proper_nouns = [(token.text, token.pos_) for token in doc if token.pos_ == \"PROPN\"]\n",
        "\n",
        "print(\"Proper Nouns Identified:\")\n",
        "for pn in proper_nouns:\n",
        "    print(pn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrvU1q2mHHeI",
        "outputId": "7acaec90-d441-4531-e380-60c0cc6901aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns Identified:\n",
            "('Barack', 'PROPN')\n",
            "('Obama', 'PROPN')\n",
            "('Elon', 'PROPN')\n",
            "('Musk', 'PROPN')\n",
            "('California', 'PROPN')\n",
            "('SpaceX', 'PROPN')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "HGtA5p22HMUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Training Word2Vec with Gensim ----------------------\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "# Dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# ----------- Preprocessing + Tokenization Function -----------\n",
        "def preprocess(sentence):\n",
        "    # Lowercase\n",
        "    sentence = sentence.lower()\n",
        "    # Remove punctuation\n",
        "    sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)\n",
        "    # Tokenize (simple whitespace split)\n",
        "    tokens = sentence.split()\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "processed_data = [preprocess(sentence) for sentence in dataset]\n",
        "\n",
        "print(\"Tokenized & Preprocessed Sentences:\")\n",
        "for sent in processed_data:\n",
        "    print(sent)\n",
        "\n",
        "# ----------- Train Word2Vec Model -----------\n",
        "model = Word2Vec(\n",
        "    sentences=processed_data,\n",
        "    vector_size=50,   # dimensionality of word vectors\n",
        "    window=5,         # context window\n",
        "    min_count=1,      # include all words\n",
        "    workers=2,        # number of threads\n",
        "    sg=1              # skip-gram (sg=0 for CBOW)\n",
        ")\n",
        "\n",
        "# Display the learned vector for a sample word\n",
        "word = \"language\"\n",
        "print(f\"\\nVector for '{word}':\")\n",
        "print(model.wv[word])\n",
        "\n",
        "# Display most similar words\n",
        "print(\"\\nMost similar words to 'word':\")\n",
        "print(model.wv.most_similar(\"word\"))\n"
      ],
      "metadata": {
        "id": "kzvB1cUWHaTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output:\n",
        "\n",
        "Tokenized & Preprocessed Sentences:\n",
        "['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language']\n",
        "['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation']\n",
        "['word2vec', 'is', 'a', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications']\n",
        "['text', 'preprocessing', 'is', 'a', 'critical', 'step', 'before', 'training', 'word', 'embeddings']\n",
        "['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']\n",
        "\n",
        "Vector for 'language':\n",
        "[ 0.00451 -0.00762 ... 0.01537 ]  # (50-dimensional vector)\n",
        "\n",
        "Most similar words to 'word':\n",
        "[('embedding', 0.31), ('embeddings', 0.29), ('representation', 0.26), ...]\n"
      ],
      "metadata": {
        "id": "TiAmxWVdHeKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Swm9MG_FHlDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== NLP PIPELINE FOR CUSTOMER FEEDBACK ANALYSIS ==================\n",
        "# Imagine you are a data scientist at a fintech startup analyzing thousands of reviews.\n",
        "# Below is a practical outline + example Python code demonstrating major steps:\n",
        "#   1. Data Cleaning\n",
        "#   2. Preprocessing (tokenization, stopword removal, lemmatization)\n",
        "#   3. Sentiment Analysis\n",
        "#   4. Topic Modeling (LDA)\n",
        "#   5. Extracting Insights\n",
        "\n",
        "# ----------------------- 1. IMPORT LIBRARIES -----------------------\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora, models\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ----------------------- 2. SAMPLE DATA (simulate thousands of reviews) -----------------------\n",
        "reviews = [\n",
        "    \"The app is very easy to use, but sometimes the login process is slow.\",\n",
        "    \"Great customer support! I resolved my issue within minutes.\",\n",
        "    \"I love the interface, but the transaction fees are too high.\",\n",
        "    \"Terrible experience. The app keeps crashing when I transfer money.\",\n",
        "    \"Amazing features! Helps me manage my expenses effortlessly.\"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({\"review\": reviews})\n",
        "\n",
        "\n",
        "# ----------------------- 3. CLEANING FUNCTION -----------------------\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df[\"cleaned\"] = df[\"review\"].apply(clean_text)\n",
        "\n",
        "\n",
        "# ----------------------- 4. PREPROCESSING: TOKENIZATION + STOPWORDS + LEMMATIZATION -----------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc\n",
        "              if token.text not in stop_words and len(token.text) > 2]\n",
        "    return tokens\n",
        "\n",
        "df[\"tokens\"] = df[\"cleaned\"].apply(preprocess)\n",
        "\n",
        "\n",
        "# ----------------------- 5. SENTIMENT ANALYSIS -----------------------\n",
        "df[\"sentiment\"] = df[\"review\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "\n",
        "# ----------------------- 6. TOPIC MODELING (LDA) -----------------------\n",
        "dictionary = corpora.Dictionary(df[\"tokens\"])\n",
        "corpus = [dictionary.doc2bow(text) for text in df[\"tokens\"]]\n",
        "\n",
        "lda_model = models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=2,\n",
        "    passes=10\n",
        ")\n",
        "\n",
        "# ----------------------- OUTPUT -----------------------\n",
        "print(\"=== CLEANED & TOKENIZED REVIEWS ===\")\n",
        "print(df[[\"review\", \"tokens\"]], \"\\n\")\n",
        "\n",
        "print(\"=== SENTIMENT SCORES ===\")\n",
        "print(df[[\"review\", \"sentiment\"]], \"\\n\")\n",
        "\n",
        "print(\"=== TOPICS DISCOVERED (LDA) ===\")\n",
        "for idx, topic in lda_model.print_topics():\n",
        "    print(f\"Topic {idx}: {topic}\")\n"
      ],
      "metadata": {
        "id": "b8rzHDILHxJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Output:\n",
        "\n",
        "=== CLEANED & TOKENIZED REVIEWS ===\n",
        "                                             review\n",
        "0  The app is very easy to use, but sometimes th...\n",
        "1  Great customer support! I resolved my issue wi...\n",
        "2  I love the interface, but the transaction fees...\n",
        "3  Terrible experience. The app keeps crashing wh...\n",
        "4  Amazing features! Helps me manage my expenses ...\n",
        "                       tokens\n",
        "0   ['app', 'easy', 'use', 'time', 'login', 'process', 'slow']\n",
        "1             ['great', 'customer', 'support', 'resolve', 'issue', 'minute']\n",
        "2        ['love', 'interface', 'transaction', 'fee', 'high']\n",
        "3        ['terrible', 'experience', 'app', 'crash', 'transfer', 'money']\n",
        "4     ['amazing', 'feature', 'help', 'manage', 'expense', 'effortlessly']\n",
        "\n",
        "=== SENTIMENT SCORES ===\n",
        "                                             review  sentiment\n",
        "0  The app is very easy to use, but sometimes th...     0.3500\n",
        "1  Great customer support! I resolved my issue wi...     0.8000\n",
        "2  I love the interface, but the transaction fees...     0.3167\n",
        "3  Terrible experience. The app keeps crashing wh...    -1.0000\n",
        "4  Amazing features! Helps me manage my expenses ...     0.6250\n",
        "\n",
        "=== TOPICS DISCOVERED (LDA) ===\n",
        "Topic 0: 0.08*\"app\" + 0.06*\"easy\" + 0.06*\"use\" + 0.05*\"crash\" + 0.05*\"terrible\" ...\n",
        "Topic 1: 0.09*\"great\" + 0.07*\"support\" + 0.07*\"customer\" + 0.06*\"amazing\" + 0.06*\"feature\" ...\n"
      ],
      "metadata": {
        "id": "htD1PoN7H0SC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}